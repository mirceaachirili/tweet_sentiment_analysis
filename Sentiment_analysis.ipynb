{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUzjouMrjtW9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d300c4f-54d9-49ff-f767-028ede5eab00"
      },
      "source": [
        "# Import libraries for data processing \n",
        "import random as rnd\n",
        "import string\n",
        "import re\n",
        "import os\n",
        "import nltk # Natural language toolkit\n",
        "nltk.download('twitter_samples') # Sample tweets for training the model\n",
        "nltk.download('stopwords') # Words that don't affect the meaning of the sentence\n",
        "from nltk.tokenize import TweetTokenizer \n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Install and Import deep learning library\n",
        "!pip install -q -U trax \n",
        "import trax  \n",
        "# import trax.layers\n",
        "from trax import layers as tl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "\u001b[K     |████████████████████████████████| 471kB 5.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 35.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6MB 37.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 8.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.7MB 39.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 46.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4MB 24.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 348kB 47.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 48.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 49.7MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LblJoKVyStdd"
      },
      "source": [
        "# 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp6lYbQsTA5E"
      },
      "source": [
        "## 1.1 Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_1O9fSVsJnu",
        "outputId": "1bcf7724-29bb-400c-d9c7-aee26a5b15c5"
      },
      "source": [
        "# Read in positive and negative tweets examples\n",
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "print(f\"The number of positive tweets: {len(positive_tweets)}\")\n",
        "print(f\"The number of negative tweets: {len(negative_tweets)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of positive tweets: 5000\n",
            "The number of negative tweets: 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vra0rkeMse98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c295622e-6016-4e55-df3e-493b7c7711d9"
      },
      "source": [
        "# Split examples into training (80%) and validation (20%) sets\n",
        "t = int(len(positive_tweets) * 0.8) # threshold for split\n",
        "\n",
        "train_pos = positive_tweets[:t]\n",
        "val_pos = positive_tweets[t:]\n",
        "\n",
        "train_neg = negative_tweets[:t]\n",
        "val_neg = negative_tweets[t:]\n",
        "\n",
        "# Combine training data\n",
        "train_x = train_pos + train_neg\n",
        "\n",
        "# Combine validation data\n",
        "val_x = val_pos + val_neg\n",
        "\n",
        "# Create labels for training set\n",
        "train_y = np.concatenate([np.ones(len(train_pos)), np.zeros(len(train_neg))])\n",
        "\n",
        "# Create labels for validation set\n",
        "valid_y = np.concatenate([np.ones(len(val_pos)), np.zeros(len(val_neg))])\n",
        "\n",
        "print(f\"length of train_x {len(train_x)}\")\n",
        "print(f\"length of val_x {len(val_x)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of train_x 8000\n",
            "length of val_x 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdBXRgPITJ8K"
      },
      "source": [
        "## 1.2 Processing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nWRCEBoNXCj"
      },
      "source": [
        "stopwords_english = stopwords.words('english')\n",
        "\n",
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "    \n",
        "    '''\n",
        "    # remove stock market tickers like $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    # remove hyperlinks\n",
        "    tweet = re.sub(r'http\\S+', '', tweet)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "    # tokenize tweets\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "    # call porter stemmer object\n",
        "    from nltk.stem import PorterStemmer\n",
        "    stemmer = PorterStemmer()\n",
        "    # create empty list for clean tweets\n",
        "    tweets_clean = []\n",
        "    \n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and # remove stopwords\n",
        "            word not in string.punctuation): # remove punctuation\n",
        "            #tweets_clean.append(word)\n",
        "            stem_word = stemmer.stem(word) # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "    \n",
        "    return tweets_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPO5tWQ7OeHc",
        "outputId": "12778e81-dd2e-41c3-876f-61fe8eb44633"
      },
      "source": [
        "# Test process_tweet function\n",
        "print(\"Original tweet at training position 0\")\n",
        "print(train_pos[0], '\\n')\n",
        "\n",
        "print(\"Tweet at training position 0 after processing:\")\n",
        "process_tweet(train_pos[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original tweet at training position 0\n",
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :) \n",
            "\n",
            "Tweet at training position 0 after processing:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D40v8WkRTjNC"
      },
      "source": [
        "## 1.3 Build vocabulary\n",
        "\n",
        "Map each word in each tweet to an integer (an \"index\"). \n",
        "\n",
        "The vocabulary will also include some special tokens:\n",
        "- `__PAD__` : padding\n",
        "-  `</e>`: end of line\n",
        "- `__UNK__`: a token representing any word that is not in the vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neUOa0bsOqM-",
        "outputId": "4595dc72-a6ad-49ea-a189-3f069afd1d85"
      },
      "source": [
        "vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
        "\n",
        "for tweet in train_x:\n",
        "  processed_tweet = process_tweet(tweet)\n",
        "  for word in processed_tweet:\n",
        "    if word not in vocab:\n",
        "      vocab[word] = len(vocab)\n",
        "\n",
        "print(\"Total words in vocab are\",len(vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words in vocab are 9168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi7_IFYFV5GB"
      },
      "source": [
        "## 1.4 Convert tweet to tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1epmClLZTXrJ"
      },
      "source": [
        "def tweet_to_tensor(tweet, vocab, unk_token='__UNK__'):\n",
        "    '''\n",
        "    Input: \n",
        "        tweet - A string containing a tweet\n",
        "        vocab_dict - The words dictionary\n",
        "        unk_token - The special string for unknown tokens\n",
        "        verbose - Print info durign runtime\n",
        "    Output:\n",
        "        tensor_l - A python list with\n",
        "        \n",
        "    '''\n",
        "    # Process tweet into list of words\n",
        "    word_list = process_tweet(tweet)\n",
        "\n",
        "    # Initialize tensor list that will contain the unique integer for every word\n",
        "    tensor_list = []\n",
        "\n",
        "    # Get unknown token id\n",
        "    unk_id = vocab[unk_token]\n",
        "\n",
        "    for word in word_list:\n",
        "      word_id = vocab.get(word, unk_id) # Get word id, use unk_id if word doesnt exist\n",
        "      tensor_list.append(word_id) # Append word id to tensor\n",
        "\n",
        "    return tensor_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ4Ky1-pXe7C",
        "outputId": "24d1cd3e-5c8f-4ab6-e1e2-3479649b47da"
      },
      "source": [
        "print(\"Original tweet\\n\", train_pos[1])\n",
        "print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(train_pos[1], vocab=vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original tweet\n",
            " @Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
            "\n",
            "Tensor of tweet:\n",
            " [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 9, 21, 22]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Raa4p-I7X-M6"
      },
      "source": [
        "## 1.5 Create batch generator\n",
        "\n",
        "The data generator takes in the positive/negative tweets and returns a batch of training examples. It returns the model inputs, the targets (positive or negative labels) and the weight for each target (ex: this allows us to can treat some examples as more important to get right than others, but commonly this will all be 1.0). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFuo5P9AXu-M"
      },
      "source": [
        "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
        "    '''\n",
        "    Input: \n",
        "        data_pos - Set of posstive examples\n",
        "        data_neg - Set of negative examples\n",
        "        batch_size - number of samples per batch. Must be even\n",
        "        loop - True or False\n",
        "        vocab_dict - The words dictionary\n",
        "        shuffle - Shuffle the data order\n",
        "    Yield:\n",
        "        inputs - Subset of positive and negative examples\n",
        "        targets - The corresponding labels for the subset\n",
        "        example_weights - An array specifying the importance of each example\n",
        "        \n",
        "    '''     \n",
        "    # Make sure the batch size is an even number\n",
        "    # to allow an equal number of positive and negative samples\n",
        "    assert batch_size % 2 == 0\n",
        "    \n",
        "    # Number of positive examples in each batch is half of the batch size\n",
        "    # same with number of negative examples in each batch\n",
        "    n_to_take = batch_size // 2\n",
        "    \n",
        "    # Use pos_index to walk through the data_pos array\n",
        "    # same with neg_index and data_neg\n",
        "    pos_index = 0\n",
        "    neg_index = 0\n",
        "    \n",
        "    len_data_pos = len(data_pos)\n",
        "    len_data_neg = len(data_neg)\n",
        "    \n",
        "    # Get and array with the data indexes\n",
        "    pos_index_lines = list(range(len_data_pos))\n",
        "    neg_index_lines = list(range(len_data_neg))\n",
        "    \n",
        "    # Shuffle lines if shuffle is set to True\n",
        "    if shuffle:\n",
        "        rnd.shuffle(pos_index_lines)\n",
        "        rnd.shuffle(neg_index_lines)\n",
        "        \n",
        "    stop = False\n",
        "    \n",
        "    # Loop indefinitely\n",
        "    while not stop:  \n",
        "        \n",
        "        # Create a batch with positive and negative examples\n",
        "        batch = []\n",
        "        \n",
        "        # First part: Pack n_to_take positive examples\n",
        "        \n",
        "        # Start from pos_index and increment i up to n_to_take\n",
        "        for i in range(n_to_take):\n",
        "                    \n",
        "            # If the positive index goes past the positive dataset lenght,\n",
        "            if pos_index >= len_data_pos: \n",
        "                \n",
        "                # If loop is set to False, break once we reach the end of the dataset\n",
        "                if not loop:\n",
        "                    stop = True;\n",
        "                    break;\n",
        "                \n",
        "                # If user wants to keep re-using the data, reset the index\n",
        "                pos_index = 0\n",
        "                \n",
        "                if shuffle:\n",
        "                    # Shuffle the index of the positive sample\n",
        "                    rnd.shuffle(pos_index_lines)\n",
        "                    \n",
        "            # Get the tweet as pos_index\n",
        "            tweet = data_pos[pos_index_lines[pos_index]]\n",
        "            \n",
        "            # Convert the tweet into tensors of integers representing the processed words\n",
        "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "            \n",
        "            # Append the tensor to the batch list\n",
        "            batch.append(tensor)\n",
        "            \n",
        "            # Increment pos_index by one\n",
        "            pos_index = pos_index + 1\n",
        "\n",
        "\n",
        "        # Second part: Pack n_to_take negative examples\n",
        "    \n",
        "        # Using the same batch list, start from neg_index and increment i up to n_to_take\n",
        "        for i in range(n_to_take):\n",
        "            \n",
        "            # If the negative index goes past the negative dataset length,\n",
        "            if neg_index >= len_data_neg:\n",
        "                \n",
        "                # If loop is set to False, break once we reach the end of the dataset\n",
        "                if not loop:\n",
        "                    stop = True;\n",
        "                    break;\n",
        "                    \n",
        "                # If user wants to keep re-using the data, reset the index\n",
        "                neg_index = 0\n",
        "                \n",
        "                if shuffle:\n",
        "                    # Shuffle the index of the negative sample\n",
        "                    rnd.shuffle(neg_index_lines)\n",
        "            # Get the tweet as neg_index\n",
        "            tweet = data_neg[neg_index_lines[neg_index]]\n",
        "            \n",
        "            # Convert the tweet into tensors of integers representing the processed words\n",
        "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
        "            \n",
        "            # Append the tensor to the batch list\n",
        "            batch.append(tensor)\n",
        "            \n",
        "            # Increment neg_index by one\n",
        "            neg_index += 1\n",
        "\n",
        "\n",
        "        if stop:\n",
        "            break;\n",
        "\n",
        "        # Update the start index for positive data \n",
        "        # so that it's n_to_take positions after the current pos_index\n",
        "        pos_index += n_to_take\n",
        "        \n",
        "        # Update the start index for negative data \n",
        "        # so that it's n_to_take positions after the current neg_index\n",
        "        neg_index += n_to_take\n",
        "        \n",
        "        # Get the max tweet length (the length of the longest tweet) \n",
        "        # (you will pad all shorter tweets to have this length)\n",
        "        max_len = max([len(t) for t in batch]) \n",
        "        \n",
        "        # Initialize the input_l, which will \n",
        "        # store the padded versions of the tensors\n",
        "        tensor_pad_l = []\n",
        "        # Pad shorter tweets with zeros\n",
        "        for tensor in batch:\n",
        "\n",
        "            # Get the number of positions to pad for this tensor so that it will be max_len long\n",
        "            n_pad = max_len - len(tensor)\n",
        "            \n",
        "            # Generate a list of zeros, with length n_pad\n",
        "            pad_l = [0] * n_pad\n",
        "            \n",
        "            # Concatenate the tensor and the list of padded zeros\n",
        "            tensor_pad = tensor + pad_l\n",
        "            \n",
        "            # Append the padded tensor to the list of padded tensors\n",
        "            tensor_pad_l.append(tensor_pad)\n",
        "\n",
        "        # Convert the list of padded tensors to a numpy array\n",
        "        # and store this as the model inputs\n",
        "        inputs = np.array(tensor_pad_l)\n",
        "  \n",
        "        # Generate the list of targets for the positive examples (a list of ones)\n",
        "        # The length is the number of positive examples in the batch\n",
        "        target_pos = [1] * n_to_take\n",
        "        \n",
        "        # Generate the list of targets for the negative examples (a list of zeros)\n",
        "        # The length is the number of negative examples in the batch\n",
        "        target_neg = [0] * n_to_take\n",
        "        \n",
        "        # Concatenate the positve and negative targets\n",
        "        target_l = target_pos + target_neg\n",
        "        \n",
        "        # Convert the target list into a numpy array\n",
        "        targets = np.array(target_l)\n",
        "\n",
        "        # Example weights: Treat all examples equally importantly.\n",
        "        example_weights = np.ones_like(targets)\n",
        "        \n",
        "\n",
        "        yield inputs, targets, example_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj-WVrsIetey",
        "outputId": "beb0896a-1a88-4aa8-faed-9af570de5b2e"
      },
      "source": [
        "# Set the random number generator for the shuffle procedure\n",
        "rnd.seed(30) \n",
        "\n",
        "# Create the training data generator\n",
        "def train_generator(batch_size, shuffle = False):\n",
        "    return data_generator(train_pos, train_neg, batch_size, True, vocab, shuffle)\n",
        "\n",
        "# Create the validation data generator\n",
        "def val_generator(batch_size, shuffle = False):\n",
        "    return data_generator(val_pos, val_neg, batch_size, True, vocab, shuffle)\n",
        "\n",
        "# Create the validation data generator (without indefinite looping)\n",
        "def test_generator(batch_size, shuffle = False):\n",
        "    return data_generator(val_pos, val_neg, batch_size, False, vocab, shuffle)\n",
        "\n",
        "# Get a batch from the train_generator and inspect.\n",
        "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
        "\n",
        "# this will print a list of 4 tensors padded with zeros\n",
        "print(f'Inputs: {inputs}')\n",
        "print(f'Targets: {targets}')\n",
        "print(f'Example Weights: {example_weights}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inputs: [[2030 4492 3231    9    0    0    0    0    0    0    0]\n",
            " [5009  571 2025 1475 5233 3532  142 3532  132  464    9]\n",
            " [3798  111   96  587 2960 4007    0    0    0    0    0]\n",
            " [ 256 3798    0    0    0    0    0    0    0    0    0]]\n",
            "Targets: [1 1 0 0]\n",
            "Example Weights: [1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ajlDbbQeh_F",
        "outputId": "4a4f0d6c-d422-4be3-982f-1780e62e59dd"
      },
      "source": [
        "# Test the train_generator\n",
        "\n",
        "# Create a data generator for training data,\n",
        "# which produces batches of size 4 (for tensors and their respective targets)\n",
        "tmp_data_gen = train_generator(batch_size = 4)\n",
        "\n",
        "# Call the data generator to get one batch and its targets\n",
        "tmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)\n",
        "\n",
        "print(f\"The inputs shape is {tmp_inputs.shape}\")\n",
        "print(f\"The targets shape is {tmp_targets.shape}\")\n",
        "print(f\"The example weights shape is {tmp_example_weights.shape}\")\n",
        "\n",
        "for i,t in enumerate(tmp_inputs):\n",
        "    print(f\"input tensor: {t}; target {tmp_targets[i]}; example weights {tmp_example_weights[i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The inputs shape is (4, 14)\n",
            "The targets shape is (4,)\n",
            "The example weights shape is (4,)\n",
            "input tensor: [3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target 1; example weights 1\n",
            "input tensor: [10 11 12 13 14 15 16 17 18 19 20  9 21 22]; target 1; example weights 1\n",
            "input tensor: [5807 2931 3798    0    0    0    0    0    0    0    0    0    0    0]; target 0; example weights 1\n",
            "input tensor: [ 865  261 3689 5808  313 4499  571 1248 2795  333 1220 3798    0    0]; target 0; example weights 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiR8_LnToUqJ"
      },
      "source": [
        "# 2. Implement model\n",
        "\n",
        "We will create a classifier using artificial neural networks.\n",
        "For this implementation, we will use Trax library and its Serial combinator, which allows us to execute one layer after another.\n",
        "\n",
        "For example: tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4mUDfBQelXw"
      },
      "source": [
        "def classifier(vocab_size=len(vocab), embedding_dim=256, output_dim=2, mode='train'):\n",
        "\n",
        "    # Create embedding layer\n",
        "    embed_layer = tl.Embedding(vocab_size=vocab_size, \n",
        "                               d_feature=embedding_dim)\n",
        "\n",
        "    # Create mean layer\n",
        "    mean_layer = tl.Mean(axis=1) # takes the average for word embedding\n",
        "\n",
        "    # Create dense layer\n",
        "    dense_output_layer = tl.Dense(n_units=output_dim)\n",
        "\n",
        "    # Create LogSoftmax layer\n",
        "    log_softmax_layer = tl.LogSoftmax()\n",
        "\n",
        "    # Combine all layers\n",
        "    model = tl.Serial(\n",
        "              embed_layer,\n",
        "              mean_layer,\n",
        "              dense_output_layer,\n",
        "              log_softmax_layer\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "or9_HZpHqVhm",
        "outputId": "9c8d04db-36cb-4e99-e13f-02bf55f5d57f"
      },
      "source": [
        "temp_model = classifier()\n",
        "print(type(temp_model))\n",
        "display(temp_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'trax.layers.combinators.Serial'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Serial[\n",
              "  Embedding_9168_256\n",
              "  Mean\n",
              "  Dense_2\n",
              "  LogSoftmax\n",
              "]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbkeGFjHsQ6Q"
      },
      "source": [
        "# 3. Training \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R020aDGrQEML"
      },
      "source": [
        "## 3.1 Training the neural network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke31sd8EsQdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9efb33c3-dacb-4fc8-db97-adf474ba7588"
      },
      "source": [
        "from trax.supervised import training\n",
        "\n",
        "batch_size = 16\n",
        "rnd.seed(200)\n",
        "\n",
        "# Define training task\n",
        "train_task = training.TrainTask(\n",
        "    labeled_data = train_generator(batch_size=batch_size, shuffle=True),\n",
        "    loss_layer = tl.CrossEntropyLoss(),\n",
        "    optimizer = trax.optimizers.Adam(0.01),\n",
        "    n_steps_per_checkpoint = 10,\n",
        ")\n",
        "\n",
        "# Define evaluation task\n",
        "eval_task = training.EvalTask(\n",
        "    labeled_data = val_generator(batch_size=batch_size, shuffle=True),\n",
        "    metrics = [tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        ")\n",
        "\n",
        "# Define classifier model\n",
        "model = classifier()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCZ9sJv0sF7L",
        "outputId": "f050d9fb-61ce-4312-b962-dd25ec572d60"
      },
      "source": [
        "output_dir = '~/model/'\n",
        "output_dir_expand = os.path.expanduser(output_dir)\n",
        "print(output_dir_expand)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/model/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqjBQfumue3G"
      },
      "source": [
        "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
        "    '''\n",
        "    Input: \n",
        "        classifier - the model you are building\n",
        "        train_task - Training task\n",
        "        eval_task - Evaluation task\n",
        "        n_steps - the evaluation steps\n",
        "        output_dir - folder to save your files\n",
        "    Output:\n",
        "        trainer -  trax trainer\n",
        "    '''\n",
        "\n",
        "    train_loop = training.Loop(\n",
        "          classifier,\n",
        "          train_task,\n",
        "          eval_tasks=[eval_task],\n",
        "          output_dir=output_dir\n",
        "    )\n",
        "\n",
        "    train_loop.run(n_steps)\n",
        "\n",
        "    return train_loop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_dCoQT-vY6m",
        "outputId": "71342792-aa6c-4d12-91b8-c24ba97513f0"
      },
      "source": [
        "training_loop = train_model(model, train_task, eval_task, 100, output_dir_expand)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 2347522\n",
            "Step      1: Ran 1 train steps in 1.63 secs\n",
            "Step      1: train CrossEntropyLoss |  0.70103645\n",
            "Step      1: eval  CrossEntropyLoss |  0.70262021\n",
            "Step      1: eval          Accuracy |  0.43750000\n",
            "\n",
            "Step     10: Ran 9 train steps in 8.06 secs\n",
            "Step     10: train CrossEntropyLoss |  0.63601637\n",
            "Step     10: eval  CrossEntropyLoss |  0.56654179\n",
            "Step     10: eval          Accuracy |  0.81250000\n",
            "\n",
            "Step     20: Ran 10 train steps in 2.91 secs\n",
            "Step     20: train CrossEntropyLoss |  0.44222397\n",
            "Step     20: eval  CrossEntropyLoss |  0.21864718\n",
            "Step     20: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step     30: Ran 10 train steps in 2.61 secs\n",
            "Step     30: train CrossEntropyLoss |  0.20902090\n",
            "Step     30: eval  CrossEntropyLoss |  0.08811120\n",
            "Step     30: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step     40: Ran 10 train steps in 0.87 secs\n",
            "Step     40: train CrossEntropyLoss |  0.09095457\n",
            "Step     40: eval  CrossEntropyLoss |  0.03796640\n",
            "Step     40: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step     50: Ran 10 train steps in 2.88 secs\n",
            "Step     50: train CrossEntropyLoss |  0.09648214\n",
            "Step     50: eval  CrossEntropyLoss |  0.00340497\n",
            "Step     50: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step     60: Ran 10 train steps in 0.89 secs\n",
            "Step     60: train CrossEntropyLoss |  0.03062604\n",
            "Step     60: eval  CrossEntropyLoss |  0.00285852\n",
            "Step     60: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step     70: Ran 10 train steps in 0.92 secs\n",
            "Step     70: train CrossEntropyLoss |  0.03173086\n",
            "Step     70: eval  CrossEntropyLoss |  0.01907665\n",
            "Step     70: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step     80: Ran 10 train steps in 0.93 secs\n",
            "Step     80: train CrossEntropyLoss |  0.01947409\n",
            "Step     80: eval  CrossEntropyLoss |  0.00807641\n",
            "Step     80: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step     90: Ran 10 train steps in 0.96 secs\n",
            "Step     90: train CrossEntropyLoss |  0.00913573\n",
            "Step     90: eval  CrossEntropyLoss |  0.00400000\n",
            "Step     90: eval          Accuracy |  1.00000000\n",
            "\n",
            "Step    100: Ran 10 train steps in 0.94 secs\n",
            "Step    100: train CrossEntropyLoss |  0.01032283\n",
            "Step    100: eval  CrossEntropyLoss |  0.00078999\n",
            "Step    100: eval          Accuracy |  1.00000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3dUgWbiQK6T"
      },
      "source": [
        "## 3.2 Practice prediction making"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16aq8MHsve5X",
        "outputId": "181bf828-08ea-4e38-9e29-baf45f3ab365"
      },
      "source": [
        "# Create generator object\n",
        "tmp_train_generator = train_generator(batch_size=16)\n",
        "\n",
        "# Generate one batch of 16 embeddings\n",
        "tmp_batch = next(tmp_train_generator)\n",
        "\n",
        "# Get inputs, targets and weights from batch\n",
        "tmp_inputs, tmp_targets, tmp_weights = tmp_batch\n",
        "\n",
        "print(f\"The shape of the tweet tensors is {tmp_inputs.shape} (num of examples, length of tweet tensors)\")\n",
        "print(f\"The shape of the labels is {tmp_targets.shape}, which is the batch size.\")\n",
        "print(f\"The shape of the example_weights is {tmp_example_weights.shape}, which is the same as inputs/targets size.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the tweet tensors is (16, 15) (num of examples, length of tweet tensors)\n",
            "The shape of the labels is (16,), which is the batch size.\n",
            "The shape of the example_weights is (4,), which is the same as inputs/targets size.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zpcnktTQZ7K",
        "outputId": "3bb57bf4-03db-4f9e-f215-b9c863cb9234"
      },
      "source": [
        "# Feed the tweet tensors into the model to get a prediction\n",
        "\n",
        "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
        "\n",
        "print(f\"The prediction shape is {tmp_pred.shape}, num of tensor_tweets as rows\")\n",
        "print(\"Column 0 is the probability of a negative sentiment (class 0)\")\n",
        "print(\"Column 1 is the probability of a positive sentiment (class 1)\")\n",
        "print()\n",
        "print(\"View the prediction array\")\n",
        "tmp_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The prediction shape is (16, 2), num of tensor_tweets as rows\n",
            "Column 0 is the probability of a negative sentiment (class 0)\n",
            "Column 1 is the probability of a positive sentiment (class 1)\n",
            "\n",
            "View the prediction array\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([[-1.1202906e+01, -1.3351440e-05],\n",
              "             [-1.0113113e+01, -4.0531158e-05],\n",
              "             [-5.9028568e+00, -2.7353764e-03],\n",
              "             [-7.9232450e+00, -3.6239624e-04],\n",
              "             [-4.1797781e+00, -1.5420198e-02],\n",
              "             [-7.2388878e+00, -7.1835518e-04],\n",
              "             [-8.6851482e+00, -1.6927719e-04],\n",
              "             [-7.3744698e+00, -6.2727928e-04],\n",
              "             [-3.2093525e-03, -5.7433105e+00],\n",
              "             [-6.3610077e-04, -7.3603740e+00],\n",
              "             [-1.3709068e-03, -6.5930486e+00],\n",
              "             [-5.2452087e-06, -1.2121555e+01],\n",
              "             [-5.0520897e-04, -7.5907302e+00],\n",
              "             [-1.7967224e-03, -6.3226938e+00],\n",
              "             [-4.5390129e-03, -5.3973246e+00],\n",
              "             [-4.0984154e-04, -7.7998676e+00]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KV__opj5Qbd5",
        "outputId": "0c6c6744-637b-42f8-bca3-41f6ec32eea0"
      },
      "source": [
        "tmp_is_positive = tmp_pred[:, 1] > tmp_pred[:, 0]\n",
        "\n",
        "for i, p in enumerate(tmp_is_positive):\n",
        "    print(f\"Neg log prob {tmp_pred[i,0]:.4f}\\tPos log prob {tmp_pred[i,1]:.4f}\\t is positive? {p}\\t actual {tmp_targets[i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neg log prob -11.2029\tPos log prob -0.0000\t is positive? True\t actual 1\n",
            "Neg log prob -10.1131\tPos log prob -0.0000\t is positive? True\t actual 1\n",
            "Neg log prob -5.9029\tPos log prob -0.0027\t is positive? True\t actual 1\n",
            "Neg log prob -7.9232\tPos log prob -0.0004\t is positive? True\t actual 1\n",
            "Neg log prob -4.1798\tPos log prob -0.0154\t is positive? True\t actual 1\n",
            "Neg log prob -7.2389\tPos log prob -0.0007\t is positive? True\t actual 1\n",
            "Neg log prob -8.6851\tPos log prob -0.0002\t is positive? True\t actual 1\n",
            "Neg log prob -7.3745\tPos log prob -0.0006\t is positive? True\t actual 1\n",
            "Neg log prob -0.0032\tPos log prob -5.7433\t is positive? False\t actual 0\n",
            "Neg log prob -0.0006\tPos log prob -7.3604\t is positive? False\t actual 0\n",
            "Neg log prob -0.0014\tPos log prob -6.5930\t is positive? False\t actual 0\n",
            "Neg log prob -0.0000\tPos log prob -12.1216\t is positive? False\t actual 0\n",
            "Neg log prob -0.0005\tPos log prob -7.5907\t is positive? False\t actual 0\n",
            "Neg log prob -0.0018\tPos log prob -6.3227\t is positive? False\t actual 0\n",
            "Neg log prob -0.0045\tPos log prob -5.3973\t is positive? False\t actual 0\n",
            "Neg log prob -0.0004\tPos log prob -7.7999\t is positive? False\t actual 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ug4yxzgeSJje",
        "outputId": "a412dd7c-5c90-4d93-9188-9efc48bbdb60"
      },
      "source": [
        "# Convert tmp_is_positive to integer values\n",
        "tmp_is_positive_int = tmp_is_positive.astype('int32')\n",
        "\n",
        "print(f'Array of predictions: \\n{tmp_is_positive_int}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Array of predictions: \n",
            "[1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozEC1_0Ccz_8"
      },
      "source": [
        "# 4. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OegnAL9fc4YR"
      },
      "source": [
        "## 4.1 Computing accuracy on a single batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjOTRKsqSzbk"
      },
      "source": [
        "def compute_accuracy(preds, y, y_weights):\n",
        "    \"\"\"\n",
        "    Input: \n",
        "        preds: a tensor of shape (dim_batch, output_dim) \n",
        "        y: a tensor of shape (dim_batch, output_dim) with the true labels\n",
        "        y_weights: a n.ndarray with the a weight for each example\n",
        "    Output: \n",
        "        accuracy: a float between 0-1 \n",
        "        weighted_num_correct (np.float32): Sum of the weighted correct predictions\n",
        "        sum_weights (np.float32): Sum of the weights\n",
        "    \"\"\"\n",
        "    # Create boolean array - True if tweet is positive, False if negative\n",
        "    is_pos = preds[:, 1] > preds[:, 0]\n",
        "    \n",
        "    # Convert to np.int32 type\n",
        "    is_pos_int = is_pos.astype(np.int32)\n",
        "\n",
        "    # Compare array of predictions with true labels\n",
        "    correct_preds = (is_pos_int == y) \n",
        "\n",
        "    # Calculate the sum of weights\n",
        "    sum_weights = np.sum(y_weights)\n",
        "\n",
        "    # Convert array of correct predictions to float \n",
        "    correct_float = correct_preds.astype(np.float32)\n",
        "\n",
        "    # Multiply predictions with corresponding weights\n",
        "    weighted_correct_float = np.multiply(correct_float, y_weights)\n",
        "\n",
        "    # Sum correct weighted predictions\n",
        "    weighted_num_correct = np.sum(weighted_correct_float)\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = weighted_num_correct / sum_weights\n",
        "\n",
        "    return accuracy, weighted_num_correct, sum_weights\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OpR_JoAbbff",
        "outputId": "0f60b6d0-b331-4400-822c-2aca03f82e7d"
      },
      "source": [
        "# test your function\n",
        "tmp_val_generator = val_generator(64)\n",
        "\n",
        "# get one batch\n",
        "tmp_batch = next(tmp_val_generator)\n",
        "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
        "\n",
        "# feed the tweet tensors into the model to get a prediction\n",
        "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
        "\n",
        "tmp_acc, tmp_num_correct, tmp_num_predictions = compute_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)\n",
        "\n",
        "print(f\"Model's prediction accuracy on a single training batch is: {100 * tmp_acc}%\")\n",
        "print(f\"Weighted number of correct predictions {tmp_num_correct}; weighted number of total observations predicted {tmp_num_predictions}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's prediction accuracy on a single training batch is: 100.0%\n",
            "Weighted number of correct predictions 64.0; weighted number of total observations predicted 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka70GpYRc_cd"
      },
      "source": [
        "## 4.2 Compute accuracy on validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyfSKitRblLY"
      },
      "source": [
        "def test_model(generator, model):\n",
        "    '''\n",
        "    Input: \n",
        "        generator: an iterator instance that provides batches of inputs and targets\n",
        "        model: a model instance \n",
        "    Output: \n",
        "        accuracy: float corresponding to the accuracy\n",
        "    '''\n",
        "    \n",
        "    accuracy = 0.\n",
        "    total_num_correct = 0\n",
        "    total_num_pred = 0\n",
        "    \n",
        "    for batch in generator: \n",
        "        \n",
        "        # Retrieve the inputs from the batch\n",
        "        inputs = batch[0]\n",
        "        \n",
        "        # Retrieve the targets (actual labels) from the batch\n",
        "        targets = batch[1]\n",
        "        \n",
        "        # Retrieve the example weight.\n",
        "        example_weight = batch[2]\n",
        "\n",
        "        # Make predictions using the inputs\n",
        "        pred = training_loop.eval_model(inputs)\n",
        "        \n",
        "        # Calculate accuracy for the batch by comparing its predictions and targets\n",
        "        batch_accuracy, batch_num_correct, batch_num_pred = compute_accuracy(pred, targets, example_weight)\n",
        "        \n",
        "        # Update the total number of correct predictions\n",
        "        # by adding the number of correct predictions from this batch\n",
        "        total_num_correct += batch_num_correct\n",
        "        \n",
        "        # Update the total number of predictions \n",
        "        # by adding the number of predictions made for the batch\n",
        "        total_num_pred += batch_num_pred\n",
        "\n",
        "    # Calculate accuracy over all examples\n",
        "    accuracy = total_num_correct / total_num_pred\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrrcpDvidOC7",
        "outputId": "13ebd27e-195f-4fbb-91cb-94e6d449f1c1"
      },
      "source": [
        "model = training_loop.eval_model\n",
        "accuracy = test_model(test_generator(16), model)\n",
        "\n",
        "print(f'The accuracy of the model on the validation set is {accuracy:.4f}', )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of the model on the validation set is 0.9950\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCkW9hnKdVNa",
        "outputId": "d8ddc27f-8f2a-4a7f-f63a-89e3b8781f26"
      },
      "source": [
        "a = np.array([1,2,3])\n",
        "a[None, :]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo4Z_NrEeqvd"
      },
      "source": [
        "# 5. Testing on own input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPfSlsGZzG9N"
      },
      "source": [
        "## 5.1 Predict sentiment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5iMCzAHeZVi"
      },
      "source": [
        "def predict(sentence):\n",
        "    inputs = np.array(tweet_to_tensor(sentence, vocab))\n",
        "    \n",
        "    # Batch size 1, add dimension for batch, to work with the model\n",
        "    inputs = inputs[None, :]  \n",
        "    \n",
        "    # Predict with the model\n",
        "    preds_probs = model(inputs)\n",
        "    \n",
        "    # Turn probabilities into categories\n",
        "    preds = int(preds_probs[0, 1] > preds_probs[0, 0])\n",
        "    \n",
        "    sentiment = \"negative\"\n",
        "    if preds == 1:\n",
        "        sentiment = 'positive'\n",
        "\n",
        "    return preds, sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4WVRBlkewZL",
        "outputId": "745ea18e-606d-4a63-8d29-14e4cb492804"
      },
      "source": [
        "# Try a positive sentence\n",
        "sentence = \"The movie was great\"\n",
        "tmp_pred, tmp_sentiment = predict(sentence)\n",
        "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")\n",
        "\n",
        "print()\n",
        "# Try a negative sentence\n",
        "sentence = \"I am  uncomfortable\"\n",
        "tmp_pred, tmp_sentiment = predict(sentence)\n",
        "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")#"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sentiment of the sentence \n",
            "***\n",
            "\"The movie was great\"\n",
            "***\n",
            "is positive.\n",
            "\n",
            "The sentiment of the sentence \n",
            "***\n",
            "\"I am  uncomfortable\"\n",
            "***\n",
            "is negative.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P69SFXUZzcDE"
      },
      "source": [
        "## 5.2 Load own dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uStwjFe0ez9-"
      },
      "source": [
        "# Read csv file\n",
        "twitter_df = pd.read_csv('/twitter.csv')\n",
        "\n",
        "# Convert tweets column to Numpy array\n",
        "tweets = np.array(twitter_df['tweet'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vt32bHaZ8ly8",
        "outputId": "a152c76a-c738-4ef6-9934-0388cbd6a92b"
      },
      "source": [
        "# Preview dataset\n",
        "twitter_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>user_id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>datetime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1335684285915930630</td>\n",
              "      <td>760631850662494209</td>\n",
              "      <td>@yepicturepalace @GoingMedieval I love how lit...</td>\n",
              "      <td>2020-12-06 20:35:24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1335684305507401728</td>\n",
              "      <td>1221520201008603137</td>\n",
              "      <td>@CBSNews Translation:  Welfare queen elon musk...</td>\n",
              "      <td>2020-12-06 20:35:28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1335684309143851009</td>\n",
              "      <td>20352228</td>\n",
              "      <td>@BittrexGlobal @Apple @Tesla @amazon When will...</td>\n",
              "      <td>2020-12-06 20:35:29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1335684341553377280</td>\n",
              "      <td>581786786</td>\n",
              "      <td>@TeslaChillMode @Tesla @elonmusk Is it a priva...</td>\n",
              "      <td>2020-12-06 20:35:37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1335684360473899015</td>\n",
              "      <td>16306374</td>\n",
              "      <td>@Thoug4Thoughts @AliAbdaal @Tesla Haters gonna...</td>\n",
              "      <td>2020-12-06 20:35:42</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              tweet_id  ...             datetime\n",
              "0  1335684285915930630  ...  2020-12-06 20:35:24\n",
              "1  1335684305507401728  ...  2020-12-06 20:35:28\n",
              "2  1335684309143851009  ...  2020-12-06 20:35:29\n",
              "3  1335684341553377280  ...  2020-12-06 20:35:37\n",
              "4  1335684360473899015  ...  2020-12-06 20:35:42\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30th3WWlQVb-"
      },
      "source": [
        "# Create empty arrays to store predictions\n",
        "predictions = np.empty_like(tweets)\n",
        "sentiments = np.empty_like(tweets)\n",
        "\n",
        "# Predict sentiment\n",
        "for i in range(twitter_df.shape[0]):\n",
        "    try:\n",
        "      predictions[i], sentiments[i] = predict(tweets[i])\n",
        "    except:\n",
        "      predictions[i], sentiments[i] = np.nan, np.nan\n",
        "\n",
        "# Create dataframe with tweets and sentiment predictions\n",
        "tweets_sentiment = pd.DataFrame({\n",
        "    'tweet': tweets,\n",
        "    'prediction': predictions,\n",
        "    'sentiment': sentiments\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSZapAD98zDm",
        "outputId": "133afeed-dd3e-423d-dcb6-cc3bf3695a6e"
      },
      "source": [
        "# Tweet sentiment breakdown\n",
        "tweets_sentiment.sentiment.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "negative    1386\n",
              "positive    1215\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ofsL19V8_2J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}